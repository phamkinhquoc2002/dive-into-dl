{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neural networks (RNNs) are neural networks with hidden states. Unlike traditional feedforward neural networks, where information flows in one direction from input to output, RNNs have connections that form directed cycles, allowing them to exhibit dynamic temporal behavior. This cyclic structure enables RNNs to maintain a memory of previous inputs and use this information to influence the processing of current inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](https://media.geeksforgeeks.org/wp-content/uploads/20231204125839/What-is-Recurrent-Neural-Network-660.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There was a problem when trying to write in your cache folder (C:\\Users\\HP\\.cache\\huggingface\\hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNScratch(d2l.Module):\n",
    "    \"\"\"The RNN model implemented from scratch.\"\"\"\n",
    "    def __init__(num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.W_xh = nn.Parameter(\n",
    "            torch.randn(num_inputs, num_hiddens) * sigma\n",
    "        )\n",
    "        self.W_hh = nn.Parameter(\n",
    "            torch.randn(num_hiddens, num_hiddens) * sigma\n",
    "        )\n",
    "        self.b_h = nn.Parameter(\n",
    "            torch.zeros(num_hiddens)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward method below defines how to compute the output and hidden state at any time\n",
    "step, given the current input and the state of the model at the previous time step. Note that\n",
    "the RNN model loops through the outermost dimension of inputs, updating the hidden\n",
    "state one time step at a time. The model here uses a tanh activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(RNNScratch)\n",
    "def forward(self, inputs, state=None):\n",
    "    if state is None:\n",
    "        state = torch.zeros(\n",
    "            (inputs.shape[1], self.num_hiddens),\n",
    "            device=inputs.device\n",
    "        )\n",
    "    if state:\n",
    "        state, = state\n",
    "    outputs = []\n",
    "    for X in inputs:# Shape of inputs: (num_steps, batch_size, num_inputs)\n",
    "        state = torch.tanh(torch.matmul(X, self.W_xh) +\n",
    "                           torch.matmlu(state, self.W_hh) + self.b_h)\n",
    "        outputs.append(state)\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following RNNLMScratch class defines an RNN-based language model, where we pass\n",
    "in our RNN via the rnn argument of the __init__ method. When training language models, the inputs and outputs are from the same vocabulary. Hence, they have the same dimension, which is equal to the vocabulary size. Note that we use perplexity to evaluate the\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLMScratch(d2l.Classifier):\n",
    "    def __init__(self, rnn, vocab_size, lr=0.01):\n",
    "        super().__init()\n",
    "        self.save_hyperparameters()\n",
    "        self.init_params()\n",
    "    def __init_params(self):\n",
    "        self.W_hq = nn.Parameter(\n",
    "            torch.randn(self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma\n",
    "        )\n",
    "        self.b_q = nn.Parameter(torch.zeros(self.vocab_size))\n",
    "    def training_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('ppl', torch.exp(l), train=True)\n",
    "        return l\n",
    "    def validation_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('ppl', torch.exp(l), train=False)\n",
    "        return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with such categorical data, the most common strategy is to represent each\n",
    "item by a one-hot encoding. A one-hot encoding is a vector\n",
    "whose length is given by the size of the vocabulary 𝑁, where all entries are set to 0, except\n",
    "for the entry corresponding to our token, which is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(RNNLMScratch) #@save\n",
    "def one_hot(self, X):\n",
    "# Output shape: (num_steps, batch_size, vocab_size)\n",
    "   return F.one_hot(X.T, self.vocab_size).type(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language model uses a fully connected output layer to transform RNN outputs into\n",
    "token predictions at each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(RNNLMScratch)\n",
    "def output_layer(self, rnn_outputs):\n",
    "    outputs = [torch.matmul(H, self.W_q) for H in self.rnn.outputs]\n",
    "    return torch.stack(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(RNNLMScratch)\n",
    "def forward(self, X, state=None):\n",
    "    embs = self.one_hot(X)\n",
    "    rnn_outputs, _ = self.rnn(embs, state)\n",
    "    return self.output_layer(rnn_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Clipping is a method where the error derivative is changed or clipped to a threshold during backward propagation through the network, and using the clipped gradients to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Trainer)\n",
    "def clip_gradients(self, grad_clip_val, model):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > grad_clip_val:\n",
    "        for param in params:\n",
    "            param.grad[:] *= grad_clip_val / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a language model has been learned, we can use it not only to predict the next token\n",
    "but to continue predicting each subsequent one, treating the previously predicted token as\n",
    "though it were the next in the input. The following predict method generates a continuation, one character at a time, after\n",
    "ingesting a user-provided prefix. When looping through the characters in prefix, we\n",
    "keep passing the hidden state to the next time step but do not generate any output. This is\n",
    "called the warm-up period. After ingesting the prefix, we are now ready to begin emitting\n",
    "the subsequent characters, each of which will be fed back into the model as the input at the\n",
    "next time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(RNNLMScratch)\n",
    "def predict(self, prefix, num_preds, vocab, device=None):\n",
    "    state, outputs = None, [vocab[prefix[0]]]\n",
    "    for i in range(len(prefix) + num_preds - 1):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
